{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2dd09a2-f678-49ea-9731-f1a7104dc4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Start with pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61915d6d-e202-44c8-a7a2-6c92e615e345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(\"Spark session created: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8601340-f22e-4c71-bd3d-f8c2995af04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Understanding what a dataframe is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53ca0a8-00a9-42de-8ae2-3630b04d0153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create spark dataframe\n",
    "data = [('Anurag', 200000), ('Bhavesh', 30000), ('Chetan', 40000), ('Dhruv', 50000)]\n",
    "column = ['Name', 'Salary']\n",
    "spark_df = spark.createDataFrame(data, column)\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "566bc850-7ed9-40d9-b580-cf8ecc995f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- spark_df:pyspark.sql.connect.dataframe.DataFrame\n",
    "\n",
    "  Name: string\n",
    "  \n",
    "  Salary: long\n",
    "- df:pandas.core.frame.DataFrame\n",
    "\n",
    "  Name: object\n",
    "\n",
    "  Salary: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38f328d-5aa4-4973-b1c3-9c70a0a0ccf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [('Anurag', 200000), ('Bhavesh', 30000), ('Chetan', 40000), ('Dhruv', 50000)]\n",
    "column = ['Name', 'Salary']\n",
    "df = pd.DataFrame(data, columns=column)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d82bbf0-b575-4647-9e5f-fe15d6ee9e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6aa65a1-5587-4f4c-88c1-b8d8ba912eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('Count of rows in spark_df: ', spark_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a32ea4-35f6-414b-8fe0-986cb33eb9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Real life usecase\n",
    "These DataFrames are used for:\n",
    "\n",
    "- Ingesting data from files\n",
    "- Transforming at scale\n",
    "- Writing to Delta Tables or Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ad0f44-ad60-4412-831e-772bc7a1142e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add datatypes\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType([StructField('Name', StringType(), True),\n",
    "                     StructField('Salary', IntegerType(), True)])\n",
    "spark_df = spark.createDataFrame(data, schema)\n",
    "display(spark_df)\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc4b192c-2907-4bee-a13d-0df0659c1b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q: What's the difference between SparkSession and SparkContext?\n",
    "\n",
    "âœ… Answer:\n",
    "\n",
    "- SparkSession is the unified entry point from Spark 2.0 onwards.\n",
    "- It internally manages SparkContext, SQLContext, and HiveContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "464c101f-bd97-48c7-887e-2d66757f4c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q: What's the difference between SparkSession and SparkContext?**\n",
    "\n",
    "- **SparkSession** is the unified entry point for working with structured data in Spark (introduced in Spark 2.0). It combines the functionality of SparkContext, SQLContext, and HiveContext.\n",
    "- **SparkContext** is the core entry point for Spark functionality prior to Spark 2.0, mainly used for low-level RDD operations.\n",
    "- In modern Spark applications, you typically use `SparkSession`, which internally manages a `SparkContext`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dbe305c-b2e1-4138-ab0b-1857b496c8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q: What is HiveContext?**\n",
    "\n",
    "- **HiveContext** is a Spark SQL component that enables Spark to run SQL queries using Hive's query language (HQL), access Hive UDFs, and read Hive tables.\n",
    "- It provides support for Hive features like Hive SerDes, UDFs, and the Hive metastore.\n",
    "- As of Spark 2.0, `HiveContext` functionality is available through `SparkSession` with Hive support enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c096918-2277-4bf8-980a-7a9bfad0430a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_data = [\n",
    "    {\"Name\": \"David\", \"Salary\": 70000},\n",
    "    {\"Name\": \"Eva\", \"Salary\": 65000}\n",
    "]\n",
    "\n",
    "df_json=spark.createDataFrame(json_data)\n",
    "display(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "036e45f1-d577-4f35-9b49-28f220cc07d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Class1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
